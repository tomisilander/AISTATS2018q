\section{QUOTIENT NML SCORE}

We will now introduce a quotient normalized maximum likelihood (qNML)
criterion for learning Bayesian network structures.  While equally
efficient to compute than BDeu and fNML, it is free from
hyperparameters, and it can be proven to give equal scores to
equivalent models. Furthermore, it coincides with the actual NML score
for exponentially many models. In our empirical tests it produces
models featuring good predictive performance with significantly
simpler structures than BDeu and fNML.

Like BDeu and fNML, qNML can be expressed as a product of $n$ terms,
one for each variable, but unlike the other two, it is not based on
further partitioning the corresponding data column
\begin{eqnarray}
\label{eqn:qnmldef}
s^{qNML}(D;G) & := & \sum_{i=1}^n s^{qNML}_i(D;G)\\
& := & \sum_{i=1}^n \log \frac{P^1_{NML}(D[\cdot,(i,G_i)];G)}
                             {P^1_{NML}(D[\cdot,G_i];G)}.\nonumber
\end{eqnarray}
The trick here is to model a subset of columns as though there were no
conditional independencies among the corresponding variables $S
\subset X$.  In this case, we can collapse the $\prod_{X_i\in S} r_i$
value configurations and consider them as values of a single variable
with $\prod_{X_i\in S} r_i$ different values which can then be modeled
with a one-dimensional $P^1_{NML}$ code.  The $s^{qNML}$ does not
necessarily define a distribution for $D$, but it is easy to verify
that it coincides with the $P_{NML}(D;G)$ for all the networks that
are composed of fully connected components.  The number of such
networks equals the number of nonempty partitions of a set of $n$
elements, i.e., the $n^\text{th}$ Bell number.

\subsection {qNML is score equivalent}

The qNML yields equal scores for network structures that encode same
sets of independencies. Verma and Pearl~\cite{Verm90} showed that the
equivalent networks are exactly those who a) are same when directed
arcs are substituted by undirected ones and b) who have same
V-structures, i.e.  the variable triplets $(A,B,C)$ where both $A$ and
$B$ are parents of $C$, but there is no arc between $A$ and $B$ (in
neither direction).  Later, Chickering~\cite{Chick95} showed that all
the equivalent network structures, and only them, can be reached from
each other by reversing, one by one, so called covered arcs, i.e. the
arcs from node $A$ to $B$, for which $B$'s parents other than $A$ are
exactly the $A$'s parents ($G_B=\{A\}\cup G_A$).

We will next state this as a
theorem and sketch a proof for it. A more detailed proof appears in the
supplementary material.
\begin{theorem}
  \label{thm:scoreqv}
  Let $G$ and $G'$ be two Bayesian network structures that differ only
  by a single covered arc reversal, i.e., the arc from $A$ to $B$ in $G$
  has been reversed in $G'$ to point from $B$ to $A$, then
  $$s^{qNML}(D;G)=s^{qNML}(D;G').$$
\end{theorem}
\begin{proof}
  Now the scores for structures can be decomposed as the
  $s^{qNML}(D;G)=\sum_{i=1}^{n}s_i^{qNML}(D;G)$ and
  $s^{qNML}(D;G')=\sum_{i=1}^{n}s_i^{qNML}(D;G')$.  Since only the
  terms corresponding to the variables $A$ and $B$ in these sums are
  different, it is enough to show that sum of these two terms are
  equal for $G$ and $G'$. Since we can assume the data to be fixed we
  lighten up the notations and write
  $P^1_{NML}(i,G_i) := P^1_{NML}(D[\cdot,(i,G_i)];G)$ and
  $P^1_{NML}(G_i)   := P^1_{NML}(D[\cdot,G_i];G)$.
  \begin{eqnarray}
    \lefteqn{s_A^{qNML}(D;G)+s_B^{qNML}(D;G)} \nonumber\\
    && =\log\frac{P^1_{NML}(A,G_{A})}{P^1_{NML}(G_{A})}
            \frac{P^1_{NML}(B,G_{B})}{P^1_{NML}(G_{B})}\nonumber\\
    && =\log 1\cdot\frac{P^1_{NML}(B,G_{B})}{P^1_{NML}(G_{A})}\nonumber\\
    && =\log \frac{P^1_{NML}(B,G'_{B})}{P^1_{NML}(G'_{A})}
             \frac{P^1_{NML}(A,G'_{A})}{P^1_{NML}(G'_{B})}\nonumber\\
 && =s_A^{qNML}(D;G')+s_B^{qNML}(D;G'),\nonumber
\end{eqnarray}
  due to equations
  $\{A\}\cup G_A  = G_B$,
  $\{B\}\cup G'_B = G'_A$,
  $\{B\}\cup G_B  = \{A\} \cup G'_A$,
  and $G_A        = G'_B$.
\end{proof}

\subsection{qNML is consistent}

One important property possessed by nearly every model selection
criterion is consistency. In our context, consistency means that given
a data matrix with $N$ samples coming from a distribution faithful to
some DAG $G$, the qNML will give the highest score to true graph $G$
with a probability tending to one as $N$ increases. We will show this
by first proving that qNML is asymptotically equivalent to widely used
BIC criterion which is known to be consistent \cite{Schw78, Haug88}.

The BIC can be written as
\begin{equation}
\textnormal{BIC}(D,G) = \sum_{i = 1}^n \log P(D_i \ | \ \hat{\theta}_{i | G_i} ) - \frac{q_i(r_i - 1)}{2} \log N,
\end{equation}
where $D_i = D[\cdot, i]$ stands for the data on variable $i$,
$\hat{\theta}_{i | G_i}$ denotes the maximum likelihood parameters of
the conditional distribution of variable $i$ given its parents in
$G$. Since the both BIC and qNML scores are decomposable, we can focus
on studying the local scores. Using the definition of qNML, we have
\begin{align}\label{qnmlDef2}
s^{qNML}_i(D;G) &= \log \frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i}
  )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} \notag \\ & -(reg(N,q_i
r_i) - reg(N,q_i)).
\end{align}

Now, we observe that the first term in (\ref{qnmlDef2}) can written as
$$
\log \frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i} )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} = \log P(D_i \ | \ \hat{\theta}_{i | G_i} )
$$ which simply shows that the goodness-of-the-fit terms agree between
the criteria. Next, we consider the difference of regrets in
(\ref{qnmlDef2}) which corresponds to the penalty term of BIC. The
following theorem states that these two are asymptotically equal.

\begin{theorem}
Let $r_i$ and $q_i$ denote the number of possible values for variable
$X_i$ and the possible configurations of parents $X_{G_i}$,
respectively. As $N \to \infty$,
$$reg(N,q_i r_i) - reg(N,q_i) = \frac{q_i(r_i - 1)}{2}\log N + O(1).$$
\end{theorem}   
\begin{proof}
By considering only the leading term in (\ref{eqn:szp}), the regret
for a single multinomial variable with $m$ categories can be written
asymptotically as
$$
reg(N,m) = \frac{m-1}{2}\log N + O(1).
$$ Using this, we have
\begin{align*}
reg(N,q_i r_i)& -  reg(N,q_i) \\ 
&= \frac{q_ir_i-1}{2}\log N-\frac{q_i-1}{2}\log N + O(1) \\
&= \frac{q_ir_i-1-q_i + 1}{2}\log N + O(1) \\
&= \frac{q_i(r_i - 1)}{2} \log N + O(1).
\end{align*}
\end{proof}
This suffices in order to show that qNML is asymptotically equivalent
to BIC and, hence, consistent. The outline of this proof follows the
similar pattern as found in \cite{SilanderIJAR10} where the
consistency of fNML was proved.

\subsection{qNML equals NML for exponentially many models}
The fNML criterion equals NML criterion just for the Bayesian network
with no arcs.  We will next show that the qNML equals NML when the
connected components of the network are tournaments (complete directed
acyclic subgraphs of G).

We first note that the NML-criterion for the Bayesian networks
decomposes by the connected components. Namely, if $G$ consists of $C$
connected components $(G^1,\ldots,G^C)$ with variable sets $(V^1,\ldots,V^C)$,
the maximum likelihood for the data $D$ decomposes, and we can write
\begin{eqnarray}
  \lefteqn{P_{NML}(D;G)} \nonumber \\
  && = \frac{P(D;\hat\theta(D),G)}
            {\sum_{D_{V_1}}\ldots \sum_{D_{V_C}}\prod_{c=1}^C P(D_{V^c};\hat\theta(D_{V^c}),G)} \nonumber \\
            && = \frac{\prod_{c=1}^C P(D_{V^c};\hat\theta(D_{V^c}),G)} 
            {\prod_{c=1}^C\sum_D^cP(D_{V^c};\hat\theta(D_{V^c}),G)}.\nonumber \\
            && = \prod_{c=1}^CP_{NML}(D_{V^c};G).
\end{eqnarray}

Clearly, the qNML score also decomposes by the connected components,
so it remains to show that if the (sub)network $G$ is a tournament,
then for any data $D$, $s^{qNML}(D;G)=\log P_{NML}(D;G)$.  Due to the
score equivalence of both the NML criterion and the qNML criterion, we
may pick such a tournament $G$ that linear ordering defined by $G$
matches the ordering of the data columns, i.e., $i<j$ implies $G_i
\subset G_j$. Now from the definition~(\ref{eqn:qnmldef}) of the qNML
criterion we see that for the tournament $G$, the sum telescopes
leaving us with $s^{qNML}(D;G) = \log P^1_{NML}(D[\cdot,G];G)$, thus
it is enough to show that $P^1_{NML}(D;G)=P_{NML}(D;G)$.
This follows, since for any data vector $x$ in data $D$, the
$P^1(x;\hat\theta(D),G) = P(x;\hat\theta(D),G)$,
where we have denoted by $P^1$ the model that takes $n$-dimensional
vectors to be values of the single (collapsed) categorical variable.
Denoting prefixes of data vector $x$ by $x^{:i}$, and number of times
such a prefix appears in a $N\times n$ datamatrix $D$ by $n_D(x^{:i})$,
($n_D(x^0)=N)$, we have
\begin{eqnarray}
  P(D;\hat\theta(D),G) &=& \prod_{j=1}^{N}P(d_j;\theta(D),G) \nonumber \\
  &=& \prod_{j=1}^{N} \prod_{i=1}^{n} \frac{n_D(d_j^{:i})}{n_D(d_j^{:i-1})} \nonumber\\
  &=& \prod_{j=1}^{N} \frac{n_D(d_j^{:n})}{N} \nonumber \\
  &=& P^1(D;\hat\theta^1(D),G).
\end{eqnarray}
Since both $P^1_{NML}$ and $P_{NML}$ are defined in terms of the
maximum likelihood probabilities, the equality above implies the equality
of these distributions.

Equality established, we would like to still state the number of
different networks connected components of which are tournaments.  We
start by generating all the $p(n)$ integer partitions i.e. ways to
partition $n$ labelled into parts with different size-profiles.  For
example, for $n=4$, we have $p(4)=5$ partition profiles $[[4], [3, 1],
  [2, 2], [2, 1, 1], [1, 1, 1, 1]]$.  Each of these partition size
profiles corresponds to many different networks, apart from the last
one that corresponds just to the empty network.  We count number of
networks for one such partition size profile (and then later sum these
counts up).  For any such partition profile $(p_1,\ldots,p_k)$ we can
(think we can) count the ways we can pick the nodes to different parts
and then order each part. This leads to the product $n\choose
p_1$$p_1$!${n-p_1}\choose p_2$$p_2!$${n-p_1-p2}\choose
p_3$$p3!$$\ldots$${n-\sum_{j=1}^{k-1}p_j}\choose p_k$$p_k!$. However,
the order of different parts of the same size does not matter, so for
all groups of parts having the same size, we have to divide the
product above by the factorial of the size of such group. Notice also,
that the product above telescopes, leaving us a formula for OEIS
sequence A000262 as described by Thomas Wieder:

With $p(n) =$ the number of integer partitions of $n$, 
     $d(i)$ = the number of different parts of the $i$-th partition of $n$, $m(i,j) =$ multiplicity of the $j$-th part of the $i$-th partition of $n$,  one has: 

$$
a(n) = \sum_{i=1}^{p(n)} \frac{n!}{\prod_{j=1}^{d(i)} m(i,j)!}.
$$

HERE SOME TEXT ABOUT GROWTH RATE

\subsection{On implementation}

Collapsing many variables into a single one causes $r$ in the
equation~(\ref{eqn:szp}) to be exponentially large (in our experiments
around $2^{20}$).  This causes the $\Gamma^2$ terms to grow very large
calling for care when computing the ratios of such huge numbers. More
specifically, scoring the $i^\text{th}$ data column, i.e., computing
$s^{qNML}_i$, requires the difference $dreg(N,q_i,r_i)=reg(N,q_i) -
reg(N,r_i q_i)$, where $q$ denotes the number of values in the
collapsed parent variable.  A numerically stable approximation is:
\begin{eqnarray}
\lefteqn{dreg(N,q,r)\approx\frac{1-r^2}{18 N} q^{2}} \\
&&+ \frac{\left(q - 1\right) \sqrt{4 q - 2} + \left(1 - q r\right) \sqrt{4 q r - 2}}{6 \sqrt{N}} \nonumber\\
&&+\frac{q\left(3 N \left(r \log{r}  - \left(r - 1\right) \left(\log{N} + 1\right)\right) + r\right)}{6 N} \nonumber\\
&&+ \frac{1}{12}(\log{(r+\frac{1-r}{q+1})}- 7\log{r}).\nonumber
\end{eqnarray}
