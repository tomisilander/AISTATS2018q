\section{QUOTIENT NML SCORE}

We will now introduce a quotient normalized maximum likelihood (qNML)
criterion for learning Bayesian network structures.  While equally
efficient to compute as BDeu and fNML, it is free from
hyperparameters, and it can be proven to give equal scores to
equivalent models. Furthermore, it coincides with the actual NML score
for exponentially many models. In our empirical tests it produces
models featuring good predictive performance with significantly
simpler structures than BDeu and fNML.

Like BDeu and fNML, qNML can be expressed as a product of $n$ terms,
one for each variable, but unlike the other two, it is not based on
further partitioning the corresponding data column
\begin{eqnarray}
\label{eqn:qnmldef}
s^{qNML}(D;G) & := & \sum_{i=1}^n s^{qNML}_i(D;G)\\
& := & \sum_{i=1}^n \log \frac{P^1_{NML}(D_{i,G_i};G)}
                             {P^1_{NML}(D_{G_i};G)}.\nonumber
\end{eqnarray}
The trick here is to model a subset of columns as though there were no
conditional independencies among the corresponding variables $S
\subset X$.  In this case, we can collapse the $\prod_{X_i\in S} r_i$
value configurations and consider them as values of a single variable
with $\prod_{X_i\in S} r_i$ different values which can then be modeled
with a one-dimensional $P^1_{NML}$ code.  The $s^{qNML}$-score does
not necessarily define a distribution for $D$, but it is easy to
verify that it coincides with the $\log P_{NML}(D;G)$ for all networks
that are composed of fully connected components.  The number of such
networks is lower bounded by the number of nonempty partitions of a
set of $n$ elements, i.e., the $n^\text{th}$ Bell number.

We are now ready to prove some important properties of the qNML-score.

\subsection {qNML IS SCORE EQUIVALENT}

qNML yields equal scores for network structures that encode same sets
of independencies. Verma and Pearl~\cite{Verm90} showed that the
equivalent networks are exactly those which a) are the same when directed
arcs are substituted by undirected ones and b) which have the same
\textit{V-structures}, i.e. the variable triplets $(A,B,C)$ where both
$A$ and $B$ are parents of $C$, but there is no arc between $A$ and
$B$ (in either direction).  Later, Chickering~\cite{Chick95} showed
that all the equivalent network structures, and only those structures,
can be reached from each other by reversing, one by one, the so-called
\textit{covered arcs}, i.e. the arcs from node $A$ to $B$, for which
$B$'s parents other than $A$ are exactly  $A$'s parents
($G_B=\{A\}\cup G_A$).

We will next state this as a
theorem and sketch a proof for it. A more detailed proof appears in the
supplementary material.
\begin{theorem}
  \label{thm:scoreqv}
  Let $G$ and $G'$ be two Bayesian network structures that differ only
  by a single covered arc reversal, i.e., the arc from $A$ to $B$ in $G$
  has been reversed in $G'$ to point from $B$ to $A$, then
  $$s^{qNML}(D;G)=s^{qNML}(D;G').$$
\end{theorem}
\begin{proof}
  Now the scores for structures can be decomposed as
  $s^{qNML}(D;G)=\sum_{i=1}^{n}s_i^{qNML}(D;G)$ and
  $s^{qNML}(D;G')=\sum_{i=1}^{n}s_i^{qNML}(D;G')$.  Since only the
  terms corresponding to the variables $A$ and $B$ in these sums are
  different, it is enough to show that the sum of these two terms are
  equal for $G$ and $G'$. Since we can assume the data to be fixed we
  lighten up the notation and write
  $P^1_{NML}(i,G_i) := P^1_{NML}(D_{i,G_i};G)$ and
  $P^1_{NML}(G_i)   := P^1_{NML}(D_{G_i};G)$.
  \begin{eqnarray}
    \lefteqn{s_A^{qNML}(D;G)+s_B^{qNML}(D;G)} \nonumber\\
    && =\log\frac{P^1_{NML}(A,G_{A})}{P^1_{NML}(G_{A})}
            \frac{P^1_{NML}(B,G_{B})}{P^1_{NML}(G_{B})}\nonumber\\
    && =\log 1\cdot\frac{P^1_{NML}(B,G_{B})}{P^1_{NML}(G_{A})}\nonumber\\
    && =\log \frac{P^1_{NML}(B,G'_{B})}{P^1_{NML}(G'_{A})}
             \frac{P^1_{NML}(A,G'_{A})}{P^1_{NML}(G'_{B})}\nonumber\\
 && =s_A^{qNML}(D;G')+s_B^{qNML}(D;G'),\nonumber
\end{eqnarray}
  using the equations $\{A\}\cup G_A = G_B$, $\{B\}\cup G'_B = G'_A$,
  $\{B\}\cup G_B = \{A\} \cup G'_A$, and $G_A = G'_B$ which follow
  easily from the definition of covered arcs.
\end{proof}

\subsection{qNML IS CONSISTENT}

One important property possessed by nearly every model selection
criterion is consistency. In our context, consistency means that given
a data matrix with $N$ samples coming from a distribution faithful to
some DAG $G$, the qNML will give the highest score to true graph $G$
with a probability tending to one as $N$ increases. We will show this
by first proving that qNML is asymptotically equivalent to the widely used
BIC criterion which is known to be consistent \cite{Schw78, Haug88}.
The outline of this proof follows a similar pattern to that in
\cite{SilanderIJAR10} where the consistency of fNML was proved.


The BIC criterion can be written as
\begin{equation}\label{BIC}
\textnormal{BIC}(D;G) = \sum_{i = 1}^n \log P(D_i \ | \ \hat{\theta}_{i | G_i} ) - \frac{q_i(r_i - 1)}{2} \log N,
\end{equation}
where $\hat{\theta}_{i | G_i}$ denotes the maximum likelihood parameters of
the conditional distribution of variable $i$ given its parents in
$G$. 

Since both the BIC and qNML scores are decomposable, we can focus on
studying the local scores. We will next show that, asymptotically, the
local qNML score equals the local BIC score. This is formulated in the
following theorem:

\begin{theorem}\label{consistency}
Let $r_i$ and $q_i$ denote the number of possible values for variable
$X_i$ and its possible configurations of parents $G_i$,
respectively. As $N \to \infty$
$$
s^{qNML}_i(D;G) =  \log P(D_i \ | \ \hat{\theta}_{i | G_i} )  - \frac{q_i(r_i - 1)}{2} \log N.
$$
\end{theorem}

In order to prove this, we start with the definition of qNML and write
\begin{align}\label{qnmlDef2}
s^{qNML}_i(D;G) &= \log \frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i}
  )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} \notag \\ & -(reg(N,q_i
r_i) - reg(N,q_i)).
\end{align}

By comparing the equations (\ref{BIC}) and (\ref{qnmlDef2}), we see
that proving our clam boils down to showing two things: 1) the terms
involving the maximized likelihoods are equal and 2) the penalty terms
are asymptotically equivalent. We will formulate these as two
lemmas.

\begin{lemma}\label{MLLemma} The maximized likelihood terms in equations (\ref{BIC}) and (\ref{qnmlDef2}) are equal:    
$$
\frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i} )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} = P(D_i \ | \ \hat{\theta}_{i | G_i} ).
$$
\end{lemma}

\begin{proof}
We can write the terms on the left side of the equation as
\begin{eqnarray*}
P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i}) &=& \prod_{j,k} \Le \frac{N_{ijk}}{N}  \Ri^{N_{ijk}}, \text{ and }\\
P(D_{G_i} \ | \ \hat{\theta}_{G_i} ) &=&  \prod_{j} \Le \frac{N_{ij}}{N}  \Ri^{N_{ij}}.
\end{eqnarray*}
Here, $N_{ijk}$ denotes the number of times we observe $X_i$ taking value $k$ when its parents are in $j^\text{th}$ configuration in our data matrix $D$. Also, $N_{ij} = \sum_k N_{ijk}$ (and $\sum_{k,j}N_{ijk} = N$ for all $i$).
Therefore,
\begin{align*}
\frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i} )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} &= \frac{ \prod_{j,k} \Le \frac{N_{ijk}}{N}  \Ri^{N_{ijk}}}{\prod_{j} \Le \frac{N_{ij}}{N}  \Ri^{N_{ij}}} \\
&= \frac{ \prod_{j,k} \Le \frac{N_{ijk}}{N}  \Ri^{N_{ijk}}}{\prod_{j}\prod_{k} \Le \frac{N_{ij}}{N}  \Ri^{N_{ijk}}}\\
%% &= \prod_{j,k} \Le \frac{N_{ijk}}{N_{ij}} \Ri^{N_{ijk}} \\
&= P(D_i \ | \ \hat{\theta}_{i | G_i} ).
\end{align*} 

\end{proof}
Next, we consider the difference of regrets in
(\ref{qnmlDef2}) which corresponds to the penalty term of BIC. The
following lemma states that these two are asymptotically equal:

\begin{lemma}\label{penaltyLemma}
As $N \to \infty$,
$$reg(N,q_i r_i) - reg(N,q_i) = \frac{q_i(r_i - 1)}{2}\log N + O(1).$$   
\end{lemma}
\begin{proof}
The regret
for a single multinomial variable with $m$ categories can be written
asymptotically as
\begin{equation}\label{regretAsymp}
reg(N,m) = \frac{m-1}{2}\log N + O(1).
\end{equation}
For the more precise statement with the underlying assumptions (which are fulfilled in the multinomial case) and for the proof, we refer to \cite{Riss96a, Grun07}. Using this, we have
\begin{align*}
reg(N,q_i r_i)& -  reg(N,q_i) \\ 
&= \frac{q_ir_i-1}{2}\log N-\frac{q_i-1}{2}\log N + O(1) \\
&= \frac{q_ir_i-1-q_i + 1}{2}\log N + O(1) \\
&= \frac{q_i(r_i - 1)}{2} \log N + O(1).
\end{align*}
\end{proof}
This concludes our proof since Lemmas \ref{MLLemma} and
\ref{penaltyLemma} imply Theorem \ref{consistency}.

\subsection{qNML EQUALS NML FOR MANY MODELS}
The fNML criterion can be seen as a computationally feasible
approximation of the more desirable NML criterion.  However, the fNML
criterion equals the NML criterion only for the Bayesian network
structure with no arcs.  We will next show that the qNML criterion
equals the NML criterion for all the networks $G$ whose connected
components tournaments (i.e., complete directed acyclic subgraphs of
$G$).

\begin{theorem}
If $G$ consists of $C$ connected components $(G^1,\ldots,G^C)$ with
variable sets $(V^1,\ldots,V^C)$, then $\log P_{NML}(D;G) = s^{qNML}(D;G)$
for all data sets $D$.
\end{theorem}
\begin{proof}
We first show that the NML-criterion for a Bayesian network
decomposes by the connected components.

Because the maximum likelihood for the data $D$ decomposes,
we can write
\begin{eqnarray}
  \lefteqn{P_{NML}(D;G)} \nonumber \\
  && = \frac{P(D;\hat\theta(D),G)}
            {\sum_{D'_{V_1}}\ldots \sum_{D'_{V_C}}\prod_{c=1}^C P(D'_{V^c};\hat\theta(D'_{V^c}),G)} \nonumber \\
            && = \frac{\prod_{c=1}^C P(D_{V^c};\hat\theta(D_{V^c}),G)} 
            {\prod_{c=1}^C\sum_{D'_{V^c}}P(D'_{V^c};\hat\theta(D'_{V^c}),G)}.\nonumber \\
            && = \prod_{c=1}^CP_{NML}(D_{V^c};G).
\end{eqnarray}

Clearly, the qNML score also decomposes by the connected components,
so it remains to show that if the (sub)network $G$ is a tournament,
then for any data $D$, $s^{qNML}(D;G)=\log P_{NML}(D;G)$.  Due to the
score equivalence of the NML criterion and the qNML criterion, we may
pick a tournament $G$ such that the linear ordering defined by $G$
matches the ordering of the data columns, i.e., $i<j$ implies $G_i
\subset G_j$. Now from the definition~(\ref{eqn:qnmldef}) of the qNML
criterion we see that for the tournament $G$, the sum telescopes
leaving us with $s^{qNML}(D;G) = \log P^1_{NML}(D_G;G)$, thus
it is enough to show that $P^1_{NML}(D;G)=P_{NML}(D;G)$.  This
follows, since for any data vector $x$ in data $D$, we have
$P^1(x;\hat\theta(D),G) = P(x;\hat\theta(D),G)$, where $P^1$ denotes
the model that takes $n$-dimensional vectors to be values of the
single (collapsed) categorical variable.  Denoting prefixes of data
vector $x$ by $x^{:i}$, and the number of times such a prefix appears
on $N$ rows $[d_1,\ldots,d_N]$ of the $N\times n$ data matrix $D$ by
$N_D(x^{:i})$, (so that $N_D(x^{:0})=N)$, we have
\begin{eqnarray}
  P(D;\hat\theta(D),G) &=& \prod_{j=1}^{N}P(d_j;\hat\theta(D),G) \nonumber \\
  &=& \prod_{j=1}^{N} \prod_{i=1}^{n} \frac{N_D(d_j^{:i})}{N_D(d_j^{:i-1})} \nonumber\\
  &=& \prod_{j=1}^{N} \frac{N_D(d_j^{:n})}{N} \nonumber \\
  &=& P^1(D;\hat\theta^1(D),G).
\end{eqnarray}
Since both $P^1_{NML}$ and $P_{NML}$ are defined in terms of the
maximum likelihood probabilities, the equality above implies the equality
of these distributions.
\end{proof}

Equality established, we would like to still state the number $a(n)$
of different $n$-node networks whose connected components are
tournaments.  We start by generating all the $p(n)$ integer partitions
i.e. ways to partition $n$ labelled into parts with different
size-profiles.  For example, for $n=4$, we have $p(4)=5$ partition
profiles $[[4], [3, 1], [2, 2], [2, 1, 1], [1, 1, 1, 1]]$. Each of
these partition size profiles corresponds to many different networks,
apart from the last one that corresponds just to the empty network.
We count number of networks for one such partition size-profile (and
then later sum these counts up).  For any such partition profile
$(p_1,\ldots,p_k)$ we can count the ways we can assign the nodes to
different parts and then order each part. This leads to the product
$n\choose p_1$$p_1$!${n-p_1}\choose p_2$$p_2!$${n-p_1-p2}\choose
p_3$$p_3!$$\ldots$${n-\sum_{j=1}^{k-1}p_j}\choose p_k$$p_k!$. However,
the order of different parts of the same size does not matter, so for
all groups of parts having the same size, we have to divide the
product above by the factorial of the size of such group. Notice also,
that the product above telescopes, leaving us a formula for OEIS
sequence A000262\footnote{https://oeis.org/A000108} as
described by Thomas Wieder:

\textit{With $p(n) =$ the number of integer partitions of $n$, $d(i)$ = the
number of different parts of the $i^\text{th}$ partition of $n$,
$m(i,j) =$ multiplicity of the $j^\text{th}$ part of the $i^\text{th}$
partition of $n$, one has:}
$$
a(n) = \sum_{i=1}^{p(n)} \frac{n!}{\prod_{j=1}^{d(i)} m(i,j)!}.
$$
For example, $a(4)=\frac{4!}{1!}+\frac{4!}{1!1!}+\frac{4!}{2!}+\frac{4!}{1!2!}+\frac{4!}{4!}=73$. In general this sequence grows rapidly; $1, 1, 3, 13, 73, 501, 4051, 37633, 394353, 4596553, \ldots$.

\subsection{QNML IS REGULAR}

Suzuki \cite{Suzuki2017} defines regularity for a scoring function $Q_n(X \mid Y)$ as follows:
\begin{definition}
Assume $H_N(X \mid Y') \leq H_N(X \mid Y)$, where $Y' \subset Y.$ We say that $Q_N(\cdot \mid \cdot)$ is regular if $Q_n(X \mid Y') \geq Q_N(X \mid Y)$.
\end{definition}
In the definition, $N$ denotes the sample size, $X$ is some random variable, $Y$ denotes the proposed parent set for $X$, and $H_N(\cdot \mid \cdot)$ refers to the empirical conditional entropy. Suzuki \cite{Suzuki2017} shows that BDeu violates this principle and demonstrates that this can cause the score to prefer more complex networks even though data do not support this. Regular scores are also argued to be computationally more efficient when applied with branch-and-bound type algorithms for Bayesian network structure learning \cite{Suzuki2017_2}. 

By analyzing the penalty term of the qNML scoring function, one can prove the following statement:
\begin{theorem}
qNML score is regular.
\end{theorem}
\begin{proof}
The proof is given in the Appendix A in Supplementary Material.
\end{proof}

