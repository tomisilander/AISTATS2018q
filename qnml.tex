\section{QUOTIENT NML SCORE}

We will now introduce a quotient normalized maximum likelihood (qNML)
criterion for learning Bayesian network structures.  While equally
efficient to compute as BDeu and fNML, it is free from
hyperparameters, and it can be proven to give equal scores to
equivalent models. Furthermore, it coincides with the actual NML score
for exponentially many models. In our empirical tests it produces
models featuring good predictive performance with significantly
simpler structures than BDeu and fNML.

Like BDeu and fNML, qNML can be expressed as a product of $n$ terms,
one for each variable, but unlike the other two, it is not based on
further partitioning the corresponding data column
\begin{eqnarray}
\label{eqn:qnmldef}
s^{qNML}(D;G) & := & \sum_{i=1}^n s^{qNML}_i(D;G)\\
& := & \sum_{i=1}^n \log \frac{P^1_{NML}(D_{i,G_i};G)}
                             {P^1_{NML}(D_{G_i};G)}.\nonumber
\end{eqnarray}
The trick here is to model a subset of columns as though there were no
conditional independencies among the corresponding variables $S
\subset X$.  In this case, we can collapse the $\prod_{X_i\in S} r_i$
value configurations and consider them as values of a single variable
with $\prod_{X_i\in S} r_i$ different values which can then be modeled
with a one-dimensional $P^1_{NML}$ code.  The $s^{qNML}$-score does
not necessarily define a distribution for $D$, but it is easy to
verify that it coincides with the $\log P_{NML}(D;G)$ for all networks
that are composed of fully connected components.  The number of such
networks is lower bounded by the number of nonempty partitions of a
set of $n$ elements, i.e., the $n^\text{th}$ Bell number.

We are now ready to prove some important properties of the qNML-score.

\subsection {qNML Is Score Equivalent}

qNML yields equal scores for network structures that encode same sets
of independencies. Verma and Pearl~\cite{Verm90} showed that the
equivalent networks are exactly those which a) are the same when directed
arcs are substituted by undirected ones and b) which have the same
\textit{V-structures}, i.e. the variable triplets $(A,B,C)$ where both
$A$ and $B$ are parents of $C$, but there is no arc between $A$ and
$B$ (in either direction).  Later, Chickering~\cite{Chick95} showed
that all the equivalent network structures, and only those structures,
can be reached from each other by reversing, one by one, the so-called
\textit{covered arcs}, i.e. the arcs from node $A$ to $B$, for which
$B$'s parents other than $A$ are exactly  $A$'s parents
($G_B=\{A\}\cup G_A$).

We will next state this as a
theorem and sketch a proof for it. A more detailed proof appears in Appendix A in the Supplementary Material.
\begin{theorem}
  \label{thm:scoreqv}
  Let $G$ and $G'$ be two Bayesian network structures that differ only
  by a single covered arc reversal, i.e., the arc from $A$ to $B$ in $G$
  has been reversed in $G'$ to point from $B$ to $A$, then
  $$s^{qNML}(D;G)=s^{qNML}(D;G').$$
\end{theorem}
\begin{proof}
  Now the scores for structures can be decomposed as
  $s^{qNML}(D;G)=\sum_{i=1}^{n}s_i^{qNML}(D;G)$ and
  $s^{qNML}(D;G')=\sum_{i=1}^{n}s_i^{qNML}(D;G')$.  Since only the
  terms corresponding to the variables $A$ and $B$ in these sums are
  different, it is enough to show that the sum of these two terms are
  equal for $G$ and $G'$. Since we can assume the data to be fixed we
  lighten up the notation and write
  $P^1_{NML}(i,G_i) := P^1_{NML}(D_{i,G_i};G)$ and
  $P^1_{NML}(G_i)   := P^1_{NML}(D_{G_i};G)$.
  \begin{eqnarray}
    \lefteqn{s_A^{qNML}(D;G)+s_B^{qNML}(D;G)} \nonumber\\
    && =\log\frac{P^1_{NML}(A,G_{A})}{P^1_{NML}(G_{A})}
            \frac{P^1_{NML}(B,G_{B})}{P^1_{NML}(G_{B})}\nonumber\\
    && =\log 1\cdot\frac{P^1_{NML}(B,G_{B})}{P^1_{NML}(G_{A})}\nonumber\\
    && =\log \frac{P^1_{NML}(B,G'_{B})}{P^1_{NML}(G'_{A})}
             \frac{P^1_{NML}(A,G'_{A})}{P^1_{NML}(G'_{B})}\nonumber\\
 && =s_A^{qNML}(D;G')+s_B^{qNML}(D;G'),\nonumber
\end{eqnarray}
  using the equations $\{A\}\cup G_A = G_B$, $\{B\}\cup G'_B = G'_A$,
  $\{B\}\cup G_B = \{A\} \cup G'_A$, and $G_A = G'_B$ which follow
  easily from the definition of covered arcs.
\end{proof}

\subsection{qNML is Consistent}

One important property possessed by nearly every model selection
criterion is consistency. In our context, consistency means that given
a data matrix with $N$ samples coming from a distribution faithful to
some DAG $G$, the qNML will give the highest score to true graph $G$
with a probability tending to one as $N$ increases. We will show this
by first proving that qNML is asymptotically equivalent to the widely used
BIC criterion which is known to be consistent \cite{Schw78, Haug88}.
The outline of this proof follows a similar pattern to that in
\cite{SilanderIJAR10} where the consistency of fNML was proved.


The BIC criterion can be written as
\begin{equation}\label{BIC}
\textnormal{BIC}(D;G) = \sum_{i = 1}^n \log P(D_i \ | \ \hat{\theta}_{i | G_i} ) - \frac{q_i(r_i - 1)}{2} \log N,
\end{equation}
where $\hat{\theta}_{i | G_i}$ denotes the maximum likelihood parameters of
the conditional distribution of variable $i$ given its parents in
$G$. 

Since both the BIC and qNML scores are decomposable, we can focus on
studying the local scores. We will next show that, asymptotically, the
local qNML score equals the local BIC score. This is formulated in the
following theorem:

\begin{theorem}\label{consistency}
Let $r_i$ and $q_i$ denote the number of possible values for variable
$X_i$ and its possible configurations of parents $G_i$,
respectively. As $N \to \infty$
$$
s^{qNML}_i(D;G) =  \log P(D_i \ | \ \hat{\theta}_{i | G_i} )  - \frac{q_i(r_i - 1)}{2} \log N.
$$
\end{theorem}

In order to prove this, we start with the definition of qNML and write
\begin{align}\label{qnmlDef2}
s^{qNML}_i(D;G) &= \log \frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i}
  )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} \notag \\ & -(reg(N,q_i
r_i) - reg(N,q_i)).
\end{align}

By comparing the equations (\ref{BIC}) and (\ref{qnmlDef2}), we see
that proving our clam boils down to showing two things: 1) the terms
involving the maximized likelihoods are equal and 2) the penalty terms
are asymptotically equivalent. We will formulate these as two
lemmas.

\begin{lemma}\label{MLLemma} The maximized likelihood terms in equations (\ref{BIC}) and (\ref{qnmlDef2}) are equal:    
$$
\frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i} )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} = P(D_i \ | \ \hat{\theta}_{i | G_i} ).
$$
\end{lemma}

\begin{proof}
We can write the terms on the left side of the equation as
\begin{eqnarray*}
P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i}) &=& \prod_{j,k} \Le \frac{N_{ijk}}{N}  \Ri^{N_{ijk}}, \text{ and }\\
P(D_{G_i} \ | \ \hat{\theta}_{G_i} ) &=&  \prod_{j} \Le \frac{N_{ij}}{N}  \Ri^{N_{ij}}.
\end{eqnarray*}
Here, $N_{ijk}$ denotes the number of times we observe $X_i$ taking value $k$ when its parents are in $j^\text{th}$ configuration in our data matrix $D$. Also, $N_{ij} = \sum_k N_{ijk}$ (and $\sum_{k,j}N_{ijk} = N$ for all $i$).
Therefore,
\begin{align*}
\frac{P(D_{i, G_i} \ | \ \hat{\theta}_{i, G_i} )}{P(D_{G_i} \ | \ \hat{\theta}_{G_i} )} &= \frac{ \prod_{j,k} \Le \frac{N_{ijk}}{N}  \Ri^{N_{ijk}}}{\prod_{j} \Le \frac{N_{ij}}{N}  \Ri^{N_{ij}}} \\
&= \frac{ \prod_{j,k} \Le \frac{N_{ijk}}{N}  \Ri^{N_{ijk}}}{\prod_{j}\prod_{k} \Le \frac{N_{ij}}{N}  \Ri^{N_{ijk}}}\\
%% &= \prod_{j,k} \Le \frac{N_{ijk}}{N_{ij}} \Ri^{N_{ijk}} \\
&= P(D_i \ | \ \hat{\theta}_{i | G_i} ).
\end{align*} 

\end{proof}
Next, we consider the difference of regrets in
(\ref{qnmlDef2}) which corresponds to the penalty term of BIC. The
following lemma states that these two are asymptotically equal:

\begin{lemma}\label{penaltyLemma}
As $N \to \infty$,
$$reg(N,q_i r_i) - reg(N,q_i) = \frac{q_i(r_i - 1)}{2}\log N + O(1).$$   
\end{lemma}
\begin{proof}
The regret
for a single multinomial variable with $m$ categories can be written
asymptotically as
\begin{equation}\label{regretAsymp}
reg(N,m) = \frac{m-1}{2}\log N + O(1).
\end{equation}
For the more precise statement with the underlying assumptions (which are fulfilled in the multinomial case) and for the proof, we refer to \cite{Riss96a, Grun07}. Using this, we have
\begin{align*}
reg(N,q_i r_i)& -  reg(N,q_i) \\ 
&= \frac{q_ir_i-1}{2}\log N-\frac{q_i-1}{2}\log N + O(1) \\
&= \frac{q_ir_i-1-q_i + 1}{2}\log N + O(1) \\
&= \frac{q_i(r_i - 1)}{2} \log N + O(1).
\end{align*}
\end{proof}
This concludes our proof since Lemmas \ref{MLLemma} and
\ref{penaltyLemma} imply Theorem \ref{consistency}.

\subsection{qNML Equals NML for Many Models}
The fNML criterion can be seen as a computationally feasible
approximation of the more desirable NML criterion.  However, the fNML
criterion equals the NML criterion only for the Bayesian network
structure with no arcs.  It can be shown that the qNML criterion
equals the NML criterion for all the networks $G$ whose connected
components are tournaments (i.e., complete directed acyclic subgraphs of
$G$). These networks include the empty network, the fully connected
one and many networks in between having different complexity. While
the generating network is unlikely to be composed of tournament
components, the result increases the plausibility that qNML is a
reasonable approximation for NML in general\footnote{A claim that is
  naturally subject for further study.}.

\begin{theorem}
If $G$ consists of $C$ connected components $(G^1,\ldots,G^C)$ with
variable sets $(V^1,\ldots,V^C)$, then $\log P_{NML}(D;G) = s^{qNML}(D;G)$
for all data sets $D$.
\end{theorem}
\begin{proof}
The full proof can be found in Appendix C in the Supplementary
Material.  The proof first shows that NML decomposes for these
particular structures, so it is enough to show the equivalence for
fully connected graphs.
It further derives the number $a(n)$ of
different $n$-node networks whose connected components are
tournaments, which turns out to be the formula for OEIS sequence
A000262\footnote{https://oeis.org/A000108}.
In general this sequence grows rapidly; $1, 1, 3, 13, 73, 501, 4051,
37633, 394353, 4596553, \ldots$.
\end{proof}

\subsection{qNML is Regular}\label{sec:regularity}

Suzuki \cite{Suzuki2017} defines regularity for a scoring function $Q_n(X \mid Y)$ as follows:
\begin{definition}
Assume $H_N(X \mid Y') \leq H_N(X \mid Y)$, where $Y' \subset Y.$ We say that $Q_N(\cdot \mid \cdot)$ is regular if $Q_n(X \mid Y') \geq Q_N(X \mid Y)$.
\end{definition}
In the definition, $N$ denotes the sample size, $X$ is some random variable, $Y$ denotes the proposed parent set for $X$, and $H_N(\cdot \mid \cdot)$ refers to the empirical conditional entropy. Suzuki \cite{Suzuki2017} shows that BDeu violates this principle and demonstrates that this can cause the score to prefer more complex networks even though data do not support this. Regular scores are also argued to be computationally more efficient when applied with branch-and-bound type algorithms for Bayesian network structure learning \cite{Suzuki2017_2}. 

By analyzing the penalty term of the qNML scoring function, one can prove the following statement:
\begin{theorem}
qNML score is regular.
\end{theorem}
\begin{proof}
The proof is given in Appendix B in the Supplementary Material.
\end{proof}
As fNML criterion differs from qNML only by how the penalty term is defined, we obtain the following result with little extra work:
\begin{theorem}
fNML score is regular.
\end{theorem}
\begin{proof}
The proof is given in Appendix B in Supplementary Material.
\end{proof}

Suzuki \cite{Suzuki2017} independently introduces a Bayesian Dirichlet
quotient (BDq) score, that can also be shown to be score equivalent
and regular.  However, like BDeu, this score features a single
hyperparameter $\alpha$, and our initial studies suggest that BDq is
also very sensitive to this hyperparameter (see Appendix D in the
Supplementarty Material), the issue that was one of the main
motivations to develop a parameter-free model selection criterion like
qNML.
