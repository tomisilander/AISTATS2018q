We thank reviewers for their feedback. While there appears to be some spread in overall ratings, we are happy to see that the reviews are unanimously positive about the theoretical properties of the proposed model selection criterion, qNML, that is decomposable (unlike NML), consistent, score equivalent (unlike fNML), parameter free (unlike BDeu or BDq), regular (unlike BDeu) and possibly a reasonable approximation to the NML criterion.

Reviewer 1 gave some detailed comments that are easy to take into account in the final version of the paper. In a journal version of the paper, the comparison with the BDq, as suggested by reviewer 3, would be a natural thing to do. The issue with BDq is that it features a hyper-parameter and our preliminary results indicate that like BDeu, BDq too can be extremely sensitive to the choice of the hyperparameter. Therefore, for more meaningful comparison, the not yet thoroughly studied parameter sensitivity issue with BDq should also be addressed.

About the practicality of the proposed method: Our original motivation for looking for a new model selection criterion was actually extremely practical. We have quite a lot of experience of working with users (researchers in applied fields), and they want to use Bayesian networks due to the insight Bayesian networks provide into their domain. Unfortunately, that is where the existing methods too often appear to fail. For example, to the practitioners' disappointment, BIC often yields an empty network. This is not useful at all, say, to a social scientist that has spent months to painstakingly collect his/her (often not very big) data set. Our experiments using Structural Hamming Distance (SHD) do not fully reveal this situation, since the empty network can actually be relatively competitive in terms of SHD.

Obviously, providing more but misleading "insight" is even worse. So the models should also capture the underlying distribution, which we suggest to measure by prediction performance. What we are particularly happy about is that qNML finds a good balance by providing insight while still performing well in predictive tasks -- as pointed out by reviewer 1.

Based on these points, "it does not work in practice" (reviewer 2) is perhaps not really an accurate summary of the practicality of qNML. Besides the above points about balanced performance, it is the best method in SHD (see Figure 3) from medium sample sizes up.

Further remarks about the empirical results:

- based on the average rank, qNML is better in prediction than BIC which produces the worst predictions on large sample size settings (Table 2)
- qNML obtains always a better rank than fNML in structure learning experiments (Fig. 3).
- when averaged over all the sample sizes, fNML obtains the best rank in prediction with qNML being the second to best. However, the standard deviation of fNML's rank is 0.76 which is larger than qNML's (0.43) (data from Table 2; averages and variances not shown explicitly in the paper)
- qNML obtains this competitive performance in prediction with considerably simpler models when compared to fNML (Table 3).
- BDeu is outperformed by all the other methods in almost all prediction experiments (Table 2).
- In structure learning, qNML outperforms BDeu except in a few settings with moderate sample sizes (Fig. 3).

Clearly, none of the criteria perform the best for all sample sizes and purposes (prediction vs. SHD), which might even suggest picking different criteria for different situations and needs. We, like reviewer 3, feel that in this situation, proposing a theoretically well behaved and empirically robust criterion is indeed an interesting contribution to the field - a contribution that the Bayesian network learning community is likely to be interested to know about.
