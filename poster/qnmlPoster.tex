\documentclass[final]{beamer}

\mode<presentation>
{
  \usetheme{hiitposter}
}


\usepackage{amsmath,amsthm, amssymb,mathrsfs}
\usepackage{color}
\usepackage{times}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{url}
\usepackage[numbers]{natbib}
\usepackage[orientation=portrait,size=a0,scale=1.34,debug]{beamerposter}
\setlength{\paperwidth}{847mm}
\setlength{\paperheight}{1195mm}
%\usepackage[orientation=portrait,width = 847mm ,height = 1195mm ,scale=1.34]{beamerposter} 
\usepackage{blkarray}        

%\setbeamerfont{itemize/enumerate body}{size={\fontsize{25}{31}}}
%\setbeamerfont{itemize/enumerate subbody}{size=\footnotesize}

\graphicspath{{../qNML_images/}}

\def\newblock{\hskip .11em plus .33em minus 3.07em}

\newcommand{\heading}[1]{\alert{\large #1}\\}
\definecolor{myPurple}{RGB}{174, 206, 195}

\usepackage{fancybox}
\setlength{\fboxsep}{12pt}

\usepackage{empheq}

\definecolor{myblue}{rgb}{.85, .85, 1.0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newlength\mytemplen
\newsavebox\mytempbox

\makeatletter
\newcommand\mybluebox{%
    \@ifnextchar[%]
       {\@mybluebox}%
       {\@mybluebox[0pt]}}

\def\@mybluebox[#1]{%
    \@ifnextchar[%]
       {\@@mybluebox[#1]}%
       {\@@mybluebox[#1][0pt]}}

\def\@@mybluebox[#1][#2]#3{
    \sbox\mytempbox{#3}%
    \mytemplen\ht\mytempbox
    \advance\mytemplen #1\relax
    \ht\mytempbox\mytemplen
    \mytemplen\dp\mytempbox
    \advance\mytemplen #2\relax
    \dp\mytempbox\mytemplen
    \colorbox{myblue}{\hspace{1em}\usebox{\mytempbox}\hspace{1em}}}

\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures}
\author{Tomi Silander$^1$, Janne Lepp{\"a}-aho$^2$, Elias J{\"a}{\"a}saari$^2$, and Teemu Roos$^2$}
\institute{$^{1}$ NAVER LABS Europe, France \\ 
$^{2}$ HIIT / Department of Computer Science, University of Helsinki, Finland}

\begin{document}
\begin{frame}{}
\vskip-1.0ex

{
\setbeamercolor{block body}{bg=myPurple}
\begin{block}{Abstract}
	\large
  	We introduce an information theoretic criterion for Bayesian network
  	structure learning which we call quotient normalized maximum
	likelihood (qNML). In contrast to the closely related factorized
	normalized maximum likelihood criterion, qNML satisfies the property
	of score equivalence. It is also decomposable and completely free
	of adjustable hyperparameters. For practical computations, we identify
	a remarkably accurate approximation proposed earlier by Szpankowski
	and Weinberger. Experiments on both simulated and real data
	demonstrate that the new criterion leads to parsimonious models with
	good predictive accuracy.
\end{block}
}

\begin{block}{Background: Structure Learning of Bayesian Networks}
\begin{columns}[T]
   \begin{column}{0.3\textwidth} % first column {{{
     \heading{Bayesian Networks}
     \vspace*{12pt}
     \begin{itemize}
     \setlength\itemsep{1em}
     \item Provide a compact way to represent a joint distribution over a random vector $X = (X_1, \ldots , X_d)$. 
     \item Consist of:
     \begin{itemize}
     \item[1.] A Directed acyclic graph $G$ which encodes the dependencies between the components of $X$.
     \item[2.] Parameters $\theta = (\theta_1,\ldots,\theta_d)$, where $\theta_i$ denotes the parameters of the conditional distribution of $X_i$ given its parents $X_{G_i}$. 
     \end{itemize}
     \item Decomposition: $P(X \mid G, \theta) = \prod_{i=1}^dP(X_i\mid X_{G_i},\theta_i)$
     \end{itemize}     
   \end{column}
  
   \begin{column}{0.005\textwidth}\linethickness{0.3ex} % separator {{{
      \color{myPurple} \line(0,1){430}
   \end{column} % }}}
   
   \begin{column}{0.25\textwidth} % second column {{{
     \heading{Structure Learning}
     \vspace*{12pt}
     \begin{itemize}
     \setlength\itemsep{1em}
     \item[Data:] Each $X_i$ is a categorical variable. We observe $n$ independent samples of $X$ which are collected in a data matrix $D$ of size $n\times d$. 
     \item[Goal:] We consider a \textit{score-based} approach and seek a graph $G$ that maximizes a scoring function which evaluates how well a given graph fits the observed data.
     \item Some scoring functions: BIC, BDeu and fNML. 
   	 \end{itemize}
   \end{column}
  
   \begin{column}{0.005\textwidth}\linethickness{0.3ex} % separator {{{
      \color{myPurple} \line(0,1){430}
   \end{column} % }}}
   \begin{column}{0.3\textwidth}% third column {{{
     \heading{Scoring Functions}
     \begin{itemize}
	 \vspace*{12pt}
	 \setlength\itemsep{1em}     
     \item[BDeu] Bayesian marginal likelihood based on Dirichlet priors. Depends on a single 
     hyperparameter $\alpha >0$ called equivalent sample size. 
     \item[BIC] Maximized log-likelihood with penalty $\frac{k}{2}\log n$, where $k$ is the number of free parameters in the network. 
     \item[fNML] Factorized Normalized Maximum Likelihood. Maximized log-likelihood with penalty defined via \textit{regret} functions.
     \end{itemize}
     
   \end{column} % end of first column }}}
\end{columns}
\end{block}


\begin{block}{Quotient Normalized Maximum Likelihood Criterion}
\begin{columns}[T]

	\begin{column}{0.34\textwidth} % first column {{{
   		\heading{Motivation}
   		\begin{minipage}{0.49\textwidth}
   			\vspace*{12pt}
		   	\textcolor{teal}{BDeu}
			\begin{itemize}	    
		    \item Is very sensitive to the choice of the hyperparameter $\alpha$ \cite{cosco.uai07}.
		    \item Is not \textit{regular} \cite{Suzuki2017} (can be shown in certain situations to favour too complex models over simpler) 
		    \end{itemize}
		    \vspace*{12pt}
		    \textcolor{teal}{fNML}
			\begin{itemize}	    
		    \item Is not score equivalent: the graphs expressing the same independence statements are not scored equally.
		    \item Learned structures are often rather complex, which hampers their interpretation.
		    \end{itemize}		    
	
   		\end{minipage}
   		\begin{minipage}{0.49\textwidth}		   
		   \phantom{why vspace* does nothing??}
		   %\vspace*{12pt} 
		   \textcolor{teal}{BIC}
		    \begin{itemize}
		    \item Appears to
		require large sample sizes in order to identify appropriate
		structures \cite{cosco.pgm08a,Liu2012}. 
			\end{itemize}  
			
			
			\begin{figure}
		  	\includegraphics[width=\textwidth]{breast_cancer_npmean.pdf}
		  	\caption{Number of parameters as a function of sample size for Breast Cancer (UCI) data.}
		  	\end{figure}
   		
   		\end{minipage}
   		
   		\vspace*{12pt}
   		
   		\shadowbox{
		\parbox{0.97\textwidth}{
		\vspace*{12pt}
		The quest for a model selection criterion that would yield \textbf{more parsimonious}, \textbf{easier to interpret}, but still \textbf{predictive} Bayesian networks structures is one of the main motivations for this work.
		\vspace*{12pt}
		}}  
		
  
   	\end{column}
   
   \begin{column}{0.005\textwidth}\linethickness{0.3ex} % separator {{{
      \color{myPurple} \line(0,1){820}
   \end{column} % }}}
   
   \begin{column}{0.3\textwidth} % second column {{{
	\heading{qNML: Idea}
	\vspace*{12pt}
	\begin{itemize}
	\item Following the \textit{MDL-principle}, we would like to pick a model $G$ maximizing

	\begin{equation}
	\log P_{NML}(D;G) = \underbrace{\log P(D \mid \hat\theta , G)}_{\text{Maximized log-likelihood}} - \underbrace{\log \sum_{D'}P(D' \mid \hat  \theta,G)}_{:= \text{ regret. } \textbf{Intractable!}}, \notag
	\end{equation}%where $\hat\theta$ denotes the maximum likelihood parameters.

	\item However, for \textbf{a single categorical data vector}, regret and one dimensional $P^1_{NML}$-code can be computed.
	\item \textbf{Trick}. Recall factorization w.r.t. graph $G$:
	$$
	P(D\mid G) = \prod_{i = 1}^{d}P(D_i \mid D_{G_i}) = \prod_{i = 1}^{d}\frac{P(D_{i,G_i})}{P(D_{G_i})}
	$$
	\item Assume there are \textbf{no independencies} among $X_{G_i}$. Now, $D_{i,G_i} = $
	\vspace*{12pt}
	\end{itemize}
	\begin{minipage}{0.45\textwidth}
	\begin{align*}
	\begin{blockarray}{c|ccc}
	D_i\phantom{s} & \phantom{..}  & D_{G_i} & \phantom{..}\\
	\begin{block}{(c|ccc)}
  	1 & 1 & 1 & 1  \\
  	0 & 1 & 0 & 0 \\
  	1 & 1 &0 & 0  \\
  	1 & 1 & 1 & 1  \\
	\end{block}
	\end{blockarray} 
	\ \to \ 
	\begin{blockarray}{c}
	\phantom{...}\\
	\begin{block}{(c)}
  	0   \\
  	1  \\
  	2  \\
  	0   \\
	\end{block}
	\end{blockarray}
 	\end{align*}
	\end{minipage}\quad
	\begin{minipage}{0.45\textwidth}
	\shadowbox{
	\parbox{\textwidth}{
	\vspace*{12pt}
	\begin{itemize}
	\item We can treat $D_{i,G_i}$ and $D_{G_i}$ as values of single variables!
	\item $P(D_{i,G_i})$ and $P(D_{i,G_i})$ can be modeled using $P_{NML}^1$-code.
	\end{itemize}}}  
	\end{minipage}
	
   \end{column}
   
   \begin{column}{0.005\textwidth}\linethickness{0.3ex} % separator {{{
      \color{myPurple} \line(0,1){820}
   \end{column} % }}}
   
   
   \begin{column}{0.25\textwidth}% third column {{{
   
   	\heading{qNML: Definition \& Properties}
   	\vspace*{12pt}
   	Definition of qNML score:
    	\begin{empheq}[box={\mybluebox[4pt][4pt]}]{equation}	    		
	s^{qNML} := \sum_{i = 1}^d \log \frac{P^1_{NML}(D_{i,G_i})}{P^1_{NML}(D_{G_i})},\notag
	\end{empheq}$\log P^1_{NML}(D) = \log P(D \mid \hat{\theta}) - reg(n,r).$
	
	\begin{itemize}
	 \item $D$ is understood as  $n$ observations on single categorical variable with $r$ possible values.
	\item Accurate approximation for $reg(n,r)$ from \cite{Szpankowski2012}:
	\begin{align*}
    reg(n, r) &\approx  n\left(\log{\beta} + (\beta + 2) \log{C_\beta} - \frac{1}{C_\beta}\right) \\ 
    &- \frac{1}{2} \log{\left(C_\beta + \frac{2}{\beta}\right)},
	\end{align*}
where $\beta = \frac{r}{n}$ and $C_\beta = \frac{1}{2} + \frac{1}{2} \sqrt{1 + \frac{4}{\beta}}$.
	\end{itemize}
	
	\vspace*{12pt}
	\shadowbox{\parbox{0.85\textwidth}{
	\vspace*{12pt}
	\textbf{qNML score is:}
	\begin{itemize}	
	\item {\textbf{hyper-parameter free}}
	\item {\textbf{consistent}}
	\item {\textbf{score equivalent}}
	\item {\textbf{regular}}
	\item {\textbf{it equals NML for many models.}}
	\end{itemize}}}
	
   \end{column} 
   
   
   
\end{columns}
\end{block}

\begin{block}{Experimental Results}
  \begin{columns}[T]
    \begin{column}{0.4\textwidth}
      \heading{Structure Learning}
      
      \begin{minipage}{0.3\textwidth}
      
      \begin{itemize}
	  \item Data generated from real world DAG structures.
	  \item $5$ networks, $d  = 6,5,11,5$ and $8$, \\ edges: $ 6,4,17,4$ and $8$. 
	  \item Networks learned using exact DP-algorithm.
	  \item We measure Structural Hamming Distance (SHD).
	  %\item Average ranks (over $5$ networks and $1000$ tests) also shown.
	  \end{itemize}

	  \end{minipage}
      \begin{minipage}{0.35\textwidth}
      	\phantom{dsaasdasdasdasd}
		\begin{itemize}
	  	\item Average ranks (over $5$ networks and $1000$ tests) also shown.
	  	\end{itemize}      
      
	  \begin{figure}[h]
		\includegraphics[width=\columnwidth]{shd_all.pdf}
		\caption{SHD as a function of sample size.}
		\label{fig:all_shd}
		\end{figure}
	  \end{minipage}
	  \begin{minipage}{0.33\textwidth}
	  \begin{figure}[h]
		\includegraphics[width=\columnwidth]{shd_rank_all.pdf}
		\caption{Average ranks for each score.}
		\end{figure}
		
		\shadowbox{\parbox{0.95\textwidth}{
		qNML:
		\begin{itemize}	
		\item obtains the best ranking when $n > 300$.
		\item never has the worst average ranking.
		\end{itemize}}}		
	
	  \end{minipage}
	  
	  
	  
	\vspace*{12pt}
    \end{column}
   \begin{column}{0.005\textwidth}\linethickness{0.3ex}
      \color{myPurple} \line(0,1){630}
   \end{column} % }}}
   
    \begin{column}{0.55\textwidth}
    \heading{Prediction}
    
    \vspace*{12pt}
	\begin{minipage}{0.3\textwidth}
	\vspace*{12pt}
	\begin{itemize}
	\item 20 UCI data sets split to train and test sets.
	\item Used training fractions: $10\%, 20\% , \ldots, 90\%$.
	%\item Graphs learned using the exact algorithm.
	\item BIC, fNML and qNML use sequential predictive (sNML) parameters.
	\item BDeu uses Bayesian predictive parameter (BPP) values with $\alpha = 1$. 
	\item Scores are ranked from 1 to 4 according to test set log-loss.
	\end{itemize}	
	\end{minipage}
	\begin{minipage}{0.35\textwidth}	
		\centering
		\tiny{
		\textcolor{HIITgreen}{Table:} Average predictive performance rank over \\ different sample sizes for different scores. \\ \textbf{Best}, \underline{worst}.    
	    \begin{table}
		%\caption{Average predictive performance rank over different sample sizes for different model selection criteria in 20 different data sets.}
		\label{tbl:preds}
		\begin{center}
		\begin{tabular}{crrrrr}
		       Data
		    & $n$
		    & \multicolumn{1}{p{0.7cm}}{\centering BDeu}
		    & \multicolumn{1}{p{0.8cm}}{\centering BIC}
		    & \multicolumn{1}{p{0.9cm}}{\centering fNML}
		    & \multicolumn{1}{p{0.9cm}}{\centering qNML}\\
		       \midrule
		 PostOpe &     90 &              2.79 &     \textbf{1.20} &  \underline{3.06} &           2.94 \\
		    Iris &    150 &  \underline{2.82} &              2.37 &     \textbf{2.27} &           2.54 \\
		    Wine &    178 &  \underline{3.23} &     \textbf{1.88} &              2.67 &           2.22 \\
		   Glass &    214 &  \underline{3.61} &              3.09 &     \textbf{1.42} &           1.88 \\
		 Thyroid &    215 &              2.55 &  \underline{3.21} &     \textbf{1.80} &           2.44 \\
		 HeartSt &    270 &  \underline{3.12} &     \textbf{1.39} &              3.12 &           2.37 \\
		 BreastC &    286 &  \underline{3.09} &     \textbf{1.41} &              2.97 &           2.53 \\
		 HeartHu &    294 &  \underline{3.18} &     \textbf{1.66} &              2.90 &           2.27 \\
		 HeartCl &    303 &  \underline{3.46} &     \textbf{1.38} &              2.99 &           2.17 \\
		   Ecoli &    336 &              3.20 &  \underline{3.53} &     \textbf{1.24} &           2.04 \\
		   Liver &    345 &  \underline{3.17} &              2.39 &              2.69 &  \textbf{1.75} \\
		 Balance &    625 &  \underline{3.35} &              1.91 &     \textbf{1.59} &           3.16 \\
		 BcWisco &    699 &  \underline{3.06} &              2.03 &              2.89 &  \textbf{2.02} \\
		 Diabete &    768 &  \underline{2.91} &              2.70 &              2.68 &  \textbf{1.71} \\
		 TicTacT &    958 &  \underline{3.44} &              2.71 &     \textbf{1.31} &           2.53 \\
		   Yeast &   1484 &              2.60 &  \underline{3.76} &     \textbf{1.55} &           2.10 \\
		 Abalone &   4177 &              2.60 &  \underline{3.64} &     \textbf{1.04} &           2.72 \\
		 PageBlo &   5473 &              2.24 &  \underline{3.61} &     \textbf{1.31} &           2.83 \\
		   Adult &  32561 &              3.23 &  \underline{3.77} &     \textbf{1.00} &           2.00 \\
		 Shuttle &  58000 &     \textbf{1.44} &  \underline{3.78} &              1.56 &           3.22 \\
		\midrule		 
		 \textcolor{teal}{Mean} &  \phantom{} & \underline{2.95}  & 2.57 & \textbf{2.10}  &2.37 \\
		 \textcolor{teal}{SD}  &  \phantom{} & 0.49 & \underline{0.90} &0.76 & \textbf{0.43} \\
		\end{tabular}
		\end{center}
		\end{table}}	
	\end{minipage}
	\begin{minipage}{0.3\textwidth}
	
	
	\shadowbox{\parbox{0.95\textwidth}{
	\begin{itemize}
	\item fNML obtains on average the best rank with qNML as the runner-up.	
	\item qNML has the lowest standard deviation. 
	\end{itemize}}}
		
	We also studied the number of parameters in learned models when using $10\%$ samples for structure learning.
	\vspace*{12pt}	
	
	\shadowbox{\parbox{0.95\textwidth}{
	\begin{itemize}
	\item qNML almost always yields more parsimonious models than fNML, but more complex than BIC.	
	\end{itemize}}}
	\end{minipage}	    
          
    \end{column}
    
  \end{columns}
\end{block}

\vskip-1.0ex
\begin{columns}[T]
  \begin{column}{0.63\paperwidth}
\begin{block}{References} %{{{
%\scriptsize
%\tiny
%{
%\bibliographystyle{apalike}
%\bibliography{../cosco}
%}

\begin{tiny}
\begin{thebibliography}{}
\bibitem[Liu et~al., 2012]{Liu2012}
Liu, Z. et~al. (2012).
\newblock Empirical evaluation of scoring functions for {B}ayesian network
  model selection.
\newblock {\em BMC Bioinformatics}, 13(15):S14.

\bibitem[Silander, T. et~al., 2007]{cosco.uai07}
Silander et~al. (2007).
\newblock On sensitivity of the {MAP} {Bayesian} network structure to the
  equivalent sample size parameter.
\newblock In {\em Proceedings of {UAI--07}}.

\bibitem[Silander, T. et~al., 2008]{cosco.pgm08a}
Silander et~al. (2008).
\newblock Factorized normalized maximum likelihood criterion for learning
  {B}ayesian network structures.
\newblock In {\em Proceedings of {PGM-08}}.

\bibitem[Suzuki, 2017]{Suzuki2017}
Suzuki, J. (2017).
\newblock A theoretical analysis of the {BDeu} scores in {B}ayesian network
  structure learning.
\newblock {\em Behaviormetrika}, 44(1):97--116.

\bibitem[Szpankowski and Weinberger, 2012]{Szpankowski2012}
Szpankowski, W. and Weinberger, M.~J. (2012).
\newblock Minimax pointwise redundancy for memoryless models over large
  alphabets.
\newblock {\em IEEE Transactions on Information Theory}, 58(7):4094--4104.
\end{thebibliography}
\end{tiny}

\end{block} %}}}
  \end{column}
  \begin{column}{0.33\paperwidth}
\begin{block}{Acknowledgements} %{{{
%\scriptsize
\tiny
{
J.L., E.J. and T.R. were supported by the Academy of Finland (COIN CoE and Project \textsc{TensorML}). J.L. was supported by the DoCS doctoral programme of the University of Helsinki.
}
\end{block} %}}}
  \end{column}
\end{columns}
\end{frame}
\end{document}
 
