We thank the reviewer for all the good and stimulating questions and
suggestions.

First, it is true that the equivalent sample size parameter alpha used
for the BDeu is not properly mentioned in the paper.  It is well
established that the BDeu score is very sensitive to this parameter so
it is important to state it, and we will certainly do so in the final
version of the paper. In all the experiments we used alpha=1, which
brings us to the next questions.

A severe sensitivity of BDeu to the alpha is really a problem in
practice. Work on this has been published at UAI before at least by
Steck, and Silander et al. and recently Suzuki who showed that BDeu
adds blatantly superfluous arcs. None of the previous works really
point to a good rule of thumb for choosing alpha; there is a work for
optimizing alpha though but it adds an extra layer of complexity for
already complex learning task. In research, this sensitivity is also
annoying since experiments should be run for many values of
alpha. Data size dependent prior (as hinted by the reviewer) might be
one way, even if it ruins the beauty of the Bayesian update (you
should not change the prior when you get more data). Well, there is a
room for development there. Currently, BDeu hardly appears as a best
practical choice.

That BDeu performs well in the structure identification task when the
prior is correct is hardly surprising. While Jeffreys' prior is
different from the uniform, it still is not that different. An obvious
subject for further study is to (try to) devise a generating
mechanism that favors qNML. It is worth noticing also that
predictively qNML beats BDeu(alpha=1!) 15 times out of 20.

We acknowledge that using ranks removes useful information. In the
long version of the paper other kinds of experiments should be
conducted too. The suggestion for comparing scores has certainly its
merits. It is natural to do so for the BDeu-scores and for fNML, but
it does not make sense for the BIC, and nor does qNML define a
"marginal" distribution for the data. We are tempted, though, to try
and brutally normalize the qNML scores for comparison.  Would this
bring some additional insight, we have to thank the anonymous reviewer
for the suggestion in the long version of the paper. For bigger
networks all this is harder to do, since the scores do not give
directly the posterior probability of the model but the marginal
likelihood of the data, and the brute force normalization can not be
done due to the enormous number of models.

Lastly, the question about possible dangers of the approximation. For
small number of parents, the values can be precomputed to a table (our
current implementation actually does this). For large number of
parents, thus parent configurations, the approximation is remarkably
accurate. Already with r = 1000 (i.e., 1000 parent configurations),
the relative error is always less than 3e-7 and it gets better with
growing r. Due to the decomposability of the score, the errors do not
really accumulate very aggressively. In practice, we have not
witnessed any difference between using this approximation and the
accurate value, (which is too slow to compute in practice in the inner
loops of the learning). It is very much worth highlighting though,
that the earlier Szpankowski-approximation mentioned for example in
the fNML-literature breaks down for big r with serious consequences,
and it should not be used for qNML. Thank again for your question, we
must emphasize this in the final version of the paper.
