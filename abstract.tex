\begin{abstract}
We introduce an information theoretic criterion for Bayesian network
structure learning which we call quotient normalized maximum
likelihood (qNML). In contrast to the closely related factorized
normalized maximum likelihood criterion, qNML satisfies the property
of likelihood equivalence. It is also decomposable and completely free
of adjustable hyperparameters. For practical computations, we identify
a remarkably accurate approximation proposed earlier by Szpankowski
and Weinberger. Experiments on both simulated and real data
demonstrate that the new criterion leads to parsimonious models with
good predictive accuracy.
\end{abstract}
