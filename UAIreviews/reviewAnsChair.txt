Thank you for your question,

As a key to tables:

 - bold = the best
 - underline = the second best
 - italics = the worst
 
For a practitioner, a big part of the appeal of the Bayesian network
models is the ability to reveal interpretable structure of data
generating mechanism. For such a structure to make sense, it has to
not just look nice and interpretable, but also capture the essential
characteristics of the data generating system. This aspect is tested
by its generalization capability. This said, if we just want a good
predictor, we might as well opt for averaging over many model
structures or resorting to, say, an ensemble of deep neural network
auto-encoders. In general, we often want the ability of detecting the
relevant interpretable relationships while still maintaining
predictive power. We try to advocate qNML as such a balancing act.

For small sample sizes (N in tables 2 and 3) BIC has a nice bias that
makes it predict the best. Notice that for big sample sizes it is the
worst (for the same reason of bias). To our extensive practical
experience, when we are requested to learn a Bayesian network, too
often we have to offer "customer" an empty network if are using BIC
(In Figures 2 and 3, BIC is by far the worst). 

BDeu has its know problems with sensitivity to hyperparameters and
well documented tendency to add clearly spurious arcs. While fNML
behaves often quite OK in this regard, we sometimes see it producing
models that are still overly complex, and it lacks the desired
Markov-equivalence property.

So to answer the question when it should be used, we would say:
while some may be disappointed by qNML:s "regular second best"
character, as a hyperparameter-free, Markov equivalent criterion for
revealing structure while still maintaining reasonable predictive
performance, we find it currently the safest choice for a model
selection criterion for BNSL, a message that we believe many at the
UAI community are interested to learn.
