We thank the reviewer for bringing up an important topic of
performance.  Before addressing that, we just set the records straight
by a small note that we came up with the "quotient idea" independently
from the work by Suzuki, and that we just learned about each others'
work last year by which time both parties had already developed their
theory fully.

For the topic of the performance and the runner-up-tendency of qNML,
it was noticed very early ( "A comparison of scientific and
engineering criteria for Bayesian model selection" (1996) by David
Maxwell Chickering , David Heckerman) that in Bayesian network
structure learning one can pursue two different goals or a mixture of
these.  Our experiments indicate that BIC, while a champion in
parsimony, is clearly poor in detecting the generating structure. It
is also (due to the bias for parsimony) the worst in prediction for
the 5 biggest sample sizes (that are not even that big actually).

NML is famous for its minmax-regret formulation. In this spirit, one
can see qNML's "never lose much" (cf. runner-up) tendency also as a
form of "optimality" - winning depends on the game we are playing.





