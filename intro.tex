\section{INTRODUCTION}
\label{sec:intro}
Bayesian networks~\cite{Pear88} are popular models for presenting
multivariate statistical dependencies that may have been induced by
underlying causal mechanisms.  Techniques for learning the structure
of Bayesian networks from observational data have therefore been used
for many tasks such as discovering cell signaling pathways from
protein activity data~\cite{bn4sigpath02}, revealing the business
process structures~\cite{bn4bpmining} from transaction logs and
modeling brain-region connectivity using fMRI
data~\cite{bn4brainconnect}.

Learning the structure of statistical dependencies can be seen as a
model selection task where each model is a different hypothesis about
the conditional dependencies between sets of variables. Traditional
model selection criteria such as the Akaike information
criterion (AIC)~\cite{Akai73} and the Bayesian information
criterion (BIC)~\cite{Schw78} have also been used for the task, but recent
comparisons have not been favorable for AIC, and BIC appears to
require large sample sizes in order to identify appropriate
structures~\cite{cosco.pgm08a,Liu2012}. Traditionally, the most popular
criterion has been the marginal likelihood (usually called BDeu for
reasons explained later), but studies~\cite{cosco.uai07,Steck08} have
found this criterion to be very sensitive to hyperparameters and to
yield undesirably complex models for small sample sizes.

The information-theoretic normalized maximum likelihood (NML)
criterion~\cite{Shta87,Riss96a} would otherwise be a potential candidate
for a good criterion, but its exact calculation is likely to be
prohibitively expensive. In 2008, Silander et al. introduced a
hyperparameter free, NML inspired criterion called the factorized NML
(fNML)~\cite{cosco.pgm08a} that was shown to yield good predictive
models without such sensitivity problems. However, from the structure
learning point of view, the fNML still sometimes appears to yield
overly complex models. In this paper we introduce another NML related
criterion, the \textit{quotient NML} (qNML) that yields simpler models
without sacrificing predictive accuracy. Furthermore, unlike the fNML,
the qNML gives equal scores to structures that encode the same
independence statements. Like other common model selection criteria,
qNML is also consistent.

We next briefly introduce Bayesian networks and review the BDeu and
fNML criteria and then introduce the qNML criterion.  We also
summarize the results for 20 data sets to back up our claim that qNML
yields parsimonious models with good predictive capabilities. The
experiments with artificial data demonstrate the capability to give
high scores for the data generating model with relatively small sample
sizes.

